---
title: "Machine Learning 1"
author: "Andrea Sama (A59010582)"
date: "10/22/2021"
output: pdf_document
---
#Clustering methods: finding groupings in our data

Kmeans clustering in R is done with the `kmeans()` function.

Here we makeup some data to test and learn with.

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
data <- cbind(tmp, rev(tmp))
#making up a data set that reverses the order of the original
hist(tmp)
```
```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
data <- cbind(x=tmp, y=rev(tmp))
#making up a data set that reverses the order of the original
data
plot(data)
```

Run `kmeans()` set k (centers) to 2 (ie the number of clusters that we want) nstart 20. The thing with Kmeans is you have to tell it how many clusters you want. 
```{r}
km<-kmeans(data, centers=2, nstart=20)
km
```

>Q: How many points are in each cluster?

```{r}
km$size
```

>Q. What 'component' of your result details cluster assignment/membership?

```{r}

km$cluster

```


>Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```


>Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```


#hclust
We will use the `hclust()` function on the same data as before and see how this method works.

```{r}
#hclust(data)
```
gives an error, needs a distance. 

```{r}
hc<-hclust(dist(data))
hc
```
doesnt give a lot of information, but does provide a plot method. 

```{r}
plot(hc)
abline(h=7, col="red")
```

TO find our membership vector we need to "cut" the tree and for this we use the `cutree()` function and tell it the height to cut at.

```{r}
cutree(hc, h=7)

```

If you dont know what the height is you can use cut at k=?
use `cutree()` and state the number of k clusters that we want.

```{r}
grps<- cutree(hc, k=2)
```

```{r}
plot(data, col= grps)
```


kmeans(x, centers=?)
hclust(dist(x))

#Principal Component Analysis (PCA)
PCA projects the features onto the principal components 
The motivation is to reduce the features dimensionality while only losing a small amount of information

New low dimension axis or surfaces closest to the observations

The first PCA:Line of best fit that lies closest to data that describes the spread of the data


#PCA of UK diets

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

How many rows and columns?
```{r}
dim(x)
```

```{r}
x
```

The x is considered a column but we want it to be read as a row
One option

```{r}
x[,-1]
```
```{r}
rownames(x) <-x[, 1]
x<-x[,-1]
x
```
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```


```{r}
barplot(as.matrix(x), col=rainbow(17))
```

```{r}
mycols <- rainbow(nrow(x))
pairs(x, col=mycols, pch=16)
```

##PCA to the rescue!

Here we will use the base R function for PCA, which is called `precomp()`. 
This function wants the transpose of our data. 

```{r}

pca<-prcomp( t(x) )
summary(pca)

```

PC1 captures 67%, PC2 captures 29%, 
Cumulative proportion is the addition of the porportion of variances for the PCs


```{r}
plot(pca)
```
We want score plot (aka PCA plot). Basically a plot of PC1 vs PC2

```{r}
attributes(pca) 
```

We are after the pca$x component

```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels=colnames(x))
```
Northern Ireland is really different than these other three countries.
PCA will tell us why they are different, how much each of the different foods contributed to the differences. 

We can also examine the PCA "loadings", which tell us how much the original variables contribute to each new PC...

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot(pca$rotation[,1], las=2)
```

## One more PCA for today

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
ncol(rna.data)
```

```{r}
colnames(rna.data)
```

```{r}
nrow(rna.data)
```

```{r}
pca.rna <- prcomp( t(rna.data), scale=TRUE )
summary(pca.rna)
```

```{r}
plot(pca.rna)
```

```{r}
plot(pca.rna$x[,1:2])
text(pca.rna$x[,1:2], labels=colnames(rna.data))
```

PC1 did a good job accounting for the separation between WT and KO.
If you go to the loadings you can find what is responsible for the variation.

Lets knit to PDF and publish to github